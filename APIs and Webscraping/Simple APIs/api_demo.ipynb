{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random users data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Katarina Mathieu\n",
      "Gender: female\n",
      "Email: katarina.mathieu@example.com\n",
      "Location: 1785 Rue Dumenge, Grüsch, Genève, Switzerland 4358\n",
      "\n",
      "Name: Thomas Wang\n",
      "Gender: male\n",
      "Email: thomas.wang@example.com\n",
      "Location: 1874 Elizabeth Street, Gisborne, West Coast, New Zealand 92226\n",
      "\n",
      "Name: ستایش جعفری\n",
      "Gender: female\n",
      "Email: stysh.jaafry@example.com\n",
      "Location: 3669 فلاحی, قم, مرکزی, Iran 26120\n",
      "\n",
      "Name: Ivano Joly\n",
      "Gender: male\n",
      "Email: ivano.joly@example.com\n",
      "Location: 4665 Avenue Tony-Garnier, Lully (Fr), Vaud, Switzerland 7429\n",
      "\n",
      "Name: Naomi Barnes\n",
      "Gender: female\n",
      "Email: naomi.barnes@example.com\n",
      "Location: 1687 Mcclellan Rd, Ontario, Minnesota, United States 76299\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the API URL to fetch random user data\n",
    "url = \"https://randomuser.me/api/?results=5\"\n",
    "\n",
    "# Send an HTTP GET request to the API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    user_data = response.json()\n",
    "\n",
    "    # Print information about random users\n",
    "    for user in user_data[\"results\"]:\n",
    "        # print(user.keys())\n",
    "        print(\"Name:\", user[\"name\"][\"first\"], user[\"name\"][\"last\"])\n",
    "        print(\"Gender:\", user[\"gender\"])\n",
    "        print(\"Email:\", user[\"email\"])\n",
    "        space = \" \"\n",
    "        comma = \",\"\n",
    "        user_address = str(user[\"location\"][\"street\"][\"number\"]) \\\n",
    "                    + space + user[\"location\"][\"street\"][\"name\"] + comma \\\n",
    "                    + space + user[\"location\"][\"city\"] + comma \\\n",
    "                    + space + user[\"location\"][\"state\"] + comma \\\n",
    "                    + space + user[\"location\"][\"country\"] \\\n",
    "                    + space + str(user[\"location\"][\"postcode\"])\n",
    "        print(\"Location:\", user_address)\n",
    "        print()\n",
    "else:\n",
    "    print(\"Request failed with status code:\", response.status_code)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News titles from home page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Amazon nations fall short of pact on deforestation\n",
      "2. Amazon nations fall short of pact on deforestation\n",
      "3. Miss Universe Indonesia contestants allege sexual abuse\n",
      "4. Is bulldozer punishment trampling justice in India?\n",
      "5. Ancient lizard-like species discovered in Australia\n",
      "6. Royal prodigal son's return stirs up Thailand\n",
      "7. The Brisbane duo making classical music accessible\n",
      "8. Why falling prices in China raise concerns\n",
      "9. Is India stifling criticism by retired bureaucrats?\n",
      "10. WeWork warns of 'substantial doubt' over its future\n",
      "11. Ohio vote win for abortion-rights supporters\n",
      "12. Imran Khan barred from politics for five years\n",
      "13. Ohio vote win for abortion-rights supporters\n",
      "14. Imran Khan barred from politics for five years\n",
      "15. Which criminal case may be hardest for Trump to win?\n",
      "16. Wagner taking advantage of Niger coup - Blinken\n",
      "17. Stay healthy with 5,000 steps a day, study shows\n",
      "18. France end Morocco dream to cruise into quarters\n",
      "19. Colombia set up England match by beating Jamaica\n",
      "20. England's James apologises for World Cup red card\n",
      "21. What do you know about past 24 hours at World Cup?\n",
      "22. Mixed emotions as Nigeria exit, says ex-Super Falcon\n",
      "23. The record summer that scorched Asia\n",
      "24. BBC World News TV\n",
      "25. BBC World Service Radio\n",
      "26. Fiery ‘meteor’ over Australia probably Russian space rocket\n",
      "27. Terrifying moment car falls into India waterfall pool\n",
      "28. Investigating the 'healers' sexually abusing women\n",
      "29. China propaganda appears on London street art wall\n",
      "30. India's latest Moon mission sends first photos\n",
      "31. Fatal family lunch mystery grips Australia\n",
      "32. ‘The first thing people noticed was my cancer'\n",
      "33. I wasn't told about side-effects of antidepressants\n",
      "34. Common personal law proposal sparks fear in Indian tribes\n",
      "35. ‘Could abortion bans put my IVF at risk?’\n",
      "36. Fact-checking false claims following Niger coup\n",
      "37. Will electric flying taxis live up to their promise?\n",
      "38. The atomic \"bomb spike\" in your body\n",
      "39. Should you ditch a bad job in a week?\n",
      "40. The world's largest landlocked country\n",
      "41. Literature's most misunderstood great?\n",
      "42. The diseases that have no name\n",
      "43. The workers getting multiple bonuses\n",
      "44. Japan's innovative cooling solutions\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the BBC News website\n",
    "url = \"https://www.bbc.com/news\"\n",
    "\n",
    "# Send an HTTP GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all the news article titles using appropriate tags and class names\n",
    "article_titles = soup.find_all(\"h3\", class_=\"gs-c-promo-heading__title\")\n",
    "\n",
    "# print(article_titles)\n",
    "\n",
    "# Print the titles of the top news articles\n",
    "for index, title in enumerate(article_titles, start=1):\n",
    "    # Extract the text content of each title using get_text()\n",
    "    title_text = title.get_text(strip=True)\n",
    "    print(f\"{index}. {title_text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
