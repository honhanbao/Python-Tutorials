{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random users data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Melina Silveira\n",
      "Gender: female\n",
      "Email: melina.silveira@example.com\n",
      "Location: 3920 Beco dos Namorados, Luziânia, Bahia, Brazil 83223\n",
      "Name: Tracy Davies\n",
      "Gender: male\n",
      "Email: tracy.davies@example.com\n",
      "Location: 6332 South Street, Kildare, Donegal, Ireland 12846\n",
      "Name: Randall Vasquez\n",
      "Gender: male\n",
      "Email: randall.vasquez@example.com\n",
      "Location: 1325 Elgin St, Orange, Victoria, Australia 3568\n",
      "Name: Polyan Kryachko\n",
      "Gender: male\n",
      "Email: polyan.kryachko@example.com\n",
      "Location: 786 Prospekt Anoshkina, Kalinivka, Sumska, Ukraine 94626\n",
      "Name: Darlene Kelly\n",
      "Gender: female\n",
      "Email: darlene.kelly@example.com\n",
      "Location: 7824 Lakeview St, Allen, Rhode Island, United States 17882\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the API URL to fetch random user data\n",
    "url = \"https://randomuser.me/api/?results=5\"\n",
    "\n",
    "# Send an HTTP GET request to the API\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the JSON response\n",
    "    user_data = response.json()\n",
    "\n",
    "    # Print information about random users\n",
    "    for user in user_data[\"results\"]:\n",
    "        # print(user.keys())\n",
    "        print(\"Name:\", user[\"name\"][\"first\"], user[\"name\"][\"last\"])\n",
    "        print(\"Gender:\", user[\"gender\"])\n",
    "        print(\"Email:\", user[\"email\"])\n",
    "        space = \" \"\n",
    "        comma = \",\"\n",
    "        user_address = (\n",
    "            str(user[\"location\"][\"street\"][\"number\"]) + space + \n",
    "            user[\"location\"][\"street\"][\"name\"] + comma + space + \n",
    "            user[\"location\"][\"city\"] + comma + space + \n",
    "            user[\"location\"][\"state\"] + comma + space + \n",
    "            user[\"location\"][\"country\"] + space + \n",
    "            str(user[\"location\"][\"postcode\"])\n",
    "        )\n",
    "        print(\"Location:\", user_address)\n",
    "else:\n",
    "    print(\"Request failed with status code:\", response.status_code)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News titles from home page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Amazon nations fall short of pact on deforestation\n",
      "2. Amazon nations fall short of pact on deforestation\n",
      "3. Miss Universe Indonesia contestants allege sexual abuse\n",
      "4. Is bulldozer punishment trampling justice in India?\n",
      "5. Ancient lizard-like species discovered in Australia\n",
      "6. Royal prodigal son's return stirs up Thailand\n",
      "7. The Brisbane duo making classical music accessible\n",
      "8. Why falling prices in China raise concerns\n",
      "9. Is India stifling criticism by retired bureaucrats?\n",
      "10. WeWork warns of 'substantial doubt' over its future\n",
      "11. Ohio vote win for abortion-rights supporters\n",
      "12. Imran Khan barred from politics for five years\n",
      "13. Ohio vote win for abortion-rights supporters\n",
      "14. Imran Khan barred from politics for five years\n",
      "15. Which criminal case may be hardest for Trump to win?\n",
      "16. Wagner taking advantage of Niger coup - Blinken\n",
      "17. Stay healthy with 5,000 steps a day, study shows\n",
      "18. France end Morocco dream to cruise into quarters\n",
      "19. Colombia set up England match by beating Jamaica\n",
      "20. England's James apologises for World Cup red card\n",
      "21. What do you know about past 24 hours at World Cup?\n",
      "22. Mixed emotions as Nigeria exit, says ex-Super Falcon\n",
      "23. The record summer that scorched Asia\n",
      "24. BBC World News TV\n",
      "25. BBC World Service Radio\n",
      "26. Fiery ‘meteor’ over Australia probably Russian space rocket\n",
      "27. Terrifying moment car falls into India waterfall pool\n",
      "28. Investigating the 'healers' sexually abusing women\n",
      "29. China propaganda appears on London street art wall\n",
      "30. India's latest Moon mission sends first photos\n",
      "31. Fatal family lunch mystery grips Australia\n",
      "32. ‘The first thing people noticed was my cancer'\n",
      "33. I wasn't told about side-effects of antidepressants\n",
      "34. Common personal law proposal sparks fear in Indian tribes\n",
      "35. ‘Could abortion bans put my IVF at risk?’\n",
      "36. Fact-checking false claims following Niger coup\n",
      "37. Will electric flying taxis live up to their promise?\n",
      "38. The atomic \"bomb spike\" in your body\n",
      "39. Should you ditch a bad job in a week?\n",
      "40. The world's largest landlocked country\n",
      "41. Literature's most misunderstood great?\n",
      "42. The diseases that have no name\n",
      "43. The workers getting multiple bonuses\n",
      "44. Japan's innovative cooling solutions\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the BBC News website\n",
    "url = \"https://www.bbc.com/news\"\n",
    "\n",
    "# Send an HTTP GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Find all the news article titles using appropriate tags and class names\n",
    "article_titles = soup.find_all(\"h3\", class_=\"gs-c-promo-heading__title\")\n",
    "\n",
    "# print(article_titles)\n",
    "\n",
    "# Print the titles of the top news articles\n",
    "for index, title in enumerate(article_titles, start=1):\n",
    "    # Extract the text content of each title using get_text()\n",
    "    title_text = title.get_text(strip=True)\n",
    "    print(f\"{index}. {title_text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
